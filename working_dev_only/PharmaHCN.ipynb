{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Code Network Pharma Bot\n",
    "\n",
    "The implementation is based on this paper; which used the bAbi restaurant recommender dataset from facebook (https://research.fb.com/downloads/babi/):\n",
    "\n",
    "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning  \n",
    "Jason D. Williams, Kavosh Asadi, Geoffrey Zweig  \n",
    "https://arxiv.org/abs/1702.03274\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/166852/33999718-389cdb26-e0b9-11e7-8708-140da0803a5b.png\" >\n",
    "\n",
    "The code itself is forked from this repo https://github.com/jojonki/Hybrid-Code-Networks and has been adapted to use a new dataset. The dataset construction follows a similar process to the one used for bAbi task 5. <br>\n",
    "• Each user question has four possible phrasings and one of seven possible questions is chosen randomly for each dialogue<br>\n",
    "• The model outputs an action, some of which are masked until all the required data is gathered<br>\n",
    "• Once all the additonal questions have been asked by the bot, the user then may change their mind about some of the answers\n",
    "\n",
    "For the moment, the data is generated randomly and then stored in a SQL database, to allow scaling to a much larger dataset. Training and test sets of 1000 dialogues were generated using the same code (the test set could be made harder by holding out particular phrasings of questions).\n",
    "\n",
    "**The dialog is stored as follows:**<br>\n",
    "good morning<\\t>hello, what can I help you with today?<\\n><br>\n",
    "what do I do with drug56?<\\t>what version of drug56 is that, for example: oral or injectable?<\\n><br>\n",
    "intravenous<\\t>ok, let me look in to that<\\n><br>\n",
    "actually i meant subcutaneous<\\t>ok, is there anything else to update?<\\n><br>\n",
    "actually i meant drug41<\\t>ok, is there anything else to update?<\\n><br>\n",
    "no<\\t>answer q3: instructions for drug 41_subcutaneous<\\n><br>\n",
    "thank you<\\t>you're welcome<\\n><\\n>\n",
    "\n",
    "The first step in the code is to read a set of entities and store them in an Ordered Dictionary structure (get_entities function in utils)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "def get_entities(fpath):\n",
    "    # outputs list of entities for each type (dictionary key)\n",
    "    entities = OrderedDict({'age_groups': [], 'conditions': [], 'symptoms': [], 'severities': [], 'delivery_methods': [],\n",
    "                            'periods': [], 'strengths': [], 'units': [], 'drugs': []})\n",
    "    with open(fpath, 'r') as file:\n",
    "        # e.g. conditions<\\t>heart problems\n",
    "        lines = file.readlines()\n",
    "        for l in lines:\n",
    "            l = re.sub(r'\\n', '', l)\n",
    "            wds = l.split('\\t')\n",
    "            slot_type = wds[0] # ex) R_price\n",
    "            slot_val = wds[1] # ex) cheap\n",
    "            # if slot_type not in entities:\n",
    "            #     entities[slot_type] = []\n",
    "            if slot_type in entities:\n",
    "                if slot_val not in entities[slot_type]:\n",
    "                    entities[slot_type].append(slot_val)\n",
    "    return entities\n",
    "\n",
    "entities = get_entities('entities.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the dataset will be read to extract the set of vocabulary and actions used, stored as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_actions(ls, system_acts):\n",
    "    sys_act = ls[1]\n",
    "    sys_act = re.sub(r'drug[0-9]+', '<drug>', sys_act)\n",
    "    if sys_act.startswith('answer'): sys_act = sys_act[:9]\n",
    "    if sys_act.startswith('these are some'): sys_act = 'question symptoms'\n",
    "    if sys_act.startswith('<drug> may not be'): sys_act = 'question conditions'\n",
    "    if sys_act not in system_acts: system_acts.append(sys_act)\n",
    "    return system_acts, sys_act\n",
    "\n",
    "\n",
    "def preload(fpath, vocab, system_acts):\n",
    "    # goes through dialog and builds vocab from user utterances and also system actions\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # e.g. do you have something else<\\t>sure let me find an other option for you\n",
    "        for idx, l in enumerate(lines):\n",
    "            l = l.rstrip()\n",
    "            if l != '':\n",
    "                ls = l.split(\"\\t\")\n",
    "                uttr = ls[0].split(' ')\n",
    "                if len(ls) == 2: # includes user and system utterance\n",
    "                    for w in uttr:\n",
    "                        if w not in vocab:\n",
    "                            vocab.append(w)\n",
    "                if len(ls) == 2: # includes user and system utterance\n",
    "                    system_acts, _ = reduce_actions(ls, system_acts)\n",
    "    vocab = sorted(vocab)\n",
    "    system_acts = sorted(system_acts)\n",
    "    return vocab, system_acts\n",
    "\n",
    "fpath_train = 'dialogues_train.txt'\n",
    "fpath_test = 'dialogues_test.txt'\n",
    "SILENT = '<SILENT>'\n",
    "UNK = '<UNK>'\n",
    "system_acts = [SILENT]\n",
    "\n",
    "vocab = []\n",
    "vocab, system_acts = preload(fpath_train, vocab, system_acts) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an indexing is created for the vocabulary and action sets, UNK is added for anythin unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = dict((w, i) for i, w in enumerate(vocab, 1))\n",
    "i2w = dict((i, w) for i, w in enumerate(vocab, 1))\n",
    "w2i[UNK] = 0\n",
    "i2w[0] = UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is processed and organised into a list of environmental states which will be fed into the RNN. At each step of the sequence a state is input and the RNN outputs an action. \n",
    "\n",
    "Each state is composed of six vectors: x: user utterance, y: system action, c: context, b: Bag of Words, p: previous system action, f: action filter.\n",
    "\n",
    "At this stage, the user utterances and system actions are stored as text. The context is an indicator vector to record which of the entities have been detected from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_context(context, sentence, entities):\n",
    "    # indicator vector for all entities found in sentence\n",
    "    for idx, (ent_key, ent_vals) in enumerate(entities.items()):\n",
    "        for w in sentence:\n",
    "            if w in ent_vals:\n",
    "                context[idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag of words vector is another indicator vector recording which words are used in the utterance, according to the index defined above in w2i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow(sentence, w2i):\n",
    "    bow = [0] * len(w2i)\n",
    "    for word in sentence:\n",
    "        if word in w2i:\n",
    "            bow[w2i[word]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action filter prevents certain actions from occuring until (or after) the required information is provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_act_filter(action_size, context):\n",
    "    mask = [0] * action_size\n",
    "    ''' context: {'0age_groups': [], '1conditions': [], '2symptoms': [], '3severities': [], '4delivery_methods': [], \n",
    "        '5periods': [], '6strengths': [], '7units': [], '8drugs': []}\n",
    "    '''\n",
    "    # standard small talk\n",
    "    mask[0] = 1\n",
    "    mask[1] = 1\n",
    "    mask[9] = 1\n",
    "    mask[10] = 1\n",
    "    mask[11] = 1\n",
    "    mask[16] = 1\n",
    "\n",
    "    #clarifiaction questions if entities not found\n",
    "    if context[1] == 0:\n",
    "        mask[12] = 1\n",
    "    if context[2] == 0:\n",
    "        mask[13] = 1\n",
    "    if context[6] == 0:\n",
    "        mask[14] = 1\n",
    "    if context[4] == 0:\n",
    "        mask[15] = 1\n",
    "\n",
    "    # answers\n",
    "    if context[8] ==1:\n",
    "        mask[2] = 1\n",
    "        if context[1] == 1:\n",
    "            mask[3] = 1\n",
    "        if context[2] ==1:\n",
    "            mask[6] = 1\n",
    "        if context[4] == 1:\n",
    "            mask[4] = 1\n",
    "            mask[7] = 1\n",
    "            mask[8] = 1\n",
    "            if context[0] ==1 and context[6] ==1:\n",
    "                mask[5] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to bring that all together (and also creating an index for the actions):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 322\n",
      "action size: 17\n",
      "action_size: 17\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "    \n",
    "def load_data(fpath, entities, w2i, system_acts):\n",
    "    # inputs from get_entities and preload\n",
    "    data = []\n",
    "    with open(fpath, 'r') as f:\n",
    "        # e.g. do you have something else<\\t>sure let me find an other option for you\n",
    "        lines = f.readlines()\n",
    "        # x: user uttr, y: sys act, c: context, b: BoW, p: previous sys act, f: action filter\n",
    "        x, y, c, b, p, f = [], [], [], [], [], []\n",
    "        context = [0] * len(entities.keys())\n",
    "        for idx, l in enumerate(lines):\n",
    "            l = l.rstrip()\n",
    "            if l == '':\n",
    "                data.append((x, y, c, b, p, f))\n",
    "                # reset\n",
    "                x, y, c, b, p, f = [], [], [], [], [], []\n",
    "                context = [0] * len(entities.keys())\n",
    "            else:\n",
    "                ls = l.split(\"\\t\")\n",
    "                uttr = ls[0].split(' ')\n",
    "                update_context(context, uttr, entities)\n",
    "                act_filter = generate_act_filter(len(system_acts), context)\n",
    "                bow = get_bow(uttr, w2i)\n",
    "                sys_act = SILENT\n",
    "                if len(ls) == 2: # includes user and system utterance\n",
    "                    system_acts, sys_act = reduce_actions(ls, system_acts)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                x.append(uttr)\n",
    "                if len(y) == 0:\n",
    "                    p.append(SILENT)\n",
    "                else:\n",
    "                    p.append(y[-1])\n",
    "                y.append(sys_act)\n",
    "                c.append(copy.deepcopy(context))\n",
    "                b.append(bow)\n",
    "                f.append(act_filter)\n",
    "    return data, system_acts\n",
    "\n",
    "train_data, system_acts = load_data(fpath_train, entities, w2i, system_acts)\n",
    "test_data, system_acts = load_data(fpath_test, entities, w2i, system_acts)\n",
    "act2i = dict((act, i) for i, act in enumerate(system_acts))\n",
    "print('vocab size:', len(vocab))\n",
    "print('action size:', len(system_acts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embedding model is loaded for the words in the vocabulary from a word2vec model trained on the google news corpus (https://code.google.com/archive/p/word2vec/). To save doing this each time, the embeddings are pickled for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pre_embd_w.pickle\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from utils import save_pickle, load_pickle, load_embd_weights, load_data, to_var, add_padding\n",
    "\n",
    "# get and save embeddings for words in vocab\n",
    "# print('loading a word2vec binary...')\n",
    "# model_path = '/Users/graeme/GoogleNews-vectors-negative300.bin'\n",
    "# word2vec = KeyedVectors.load_word2vec_format('/Users/graeme/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# print('done')\n",
    "# pre_embd_w = load_embd_weights(word2vec, len(w2i), 300, w2i)\n",
    "# save_pickle(pre_embd_w, 'pre_embd_w.pickle')\n",
    "pre_embd_w = load_pickle('pre_embd_w.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the model is defined using torch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre embedding weight is set\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    '''\n",
    "    In : (N, sentence_len)\n",
    "    Out: (N, sentence_len, embd_size)\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embd_size, pre_embd_w=None, is_train_embd=False):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        if pre_embd_w is not None:\n",
    "            print('pre embedding weight is set')\n",
    "            self.embedding.weight = nn.Parameter(pre_embd_w, requires_grad=is_train_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class HybridCodeNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, hidden_size, action_size, context_size, pre_embd_w=None):\n",
    "        super(HybridCodeNetwork, self).__init__()\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = WordEmbedding(vocab_size, embd_size, pre_embd_w)\n",
    "        lstm_in_dim = embd_size + vocab_size + action_size + context_size + 1 # + 1 (Unknown vocab tag)\n",
    "        self.lstm = nn.LSTM(lstm_in_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, uttr, context, bow, prev, act_filter):\n",
    "        # uttr       : (bs, dialog_len, sentence_len)\n",
    "        # context    : (bs, dialog_len, context_dim)\n",
    "        # bow        : (bs, dialog_len, vocab_size)\n",
    "        # prev       : (bs, dialog_len, action_size)\n",
    "        # act_filter : (bs, dialog_len, action_size)\n",
    "        bs = uttr.size(0)\n",
    "        dlg_len = uttr.size(1)\n",
    "        sent_len = uttr.size(2)\n",
    "\n",
    "        # .view() is used to reshape\n",
    "        embd = self.embedding(uttr.view(bs, -1)) # (bs, dialog_len*sentence_len, embd)\n",
    "        embd = embd.view(bs, dlg_len, sent_len, -1) # (bs, dialog_len, sentence_len, embd)\n",
    "        embd = torch.mean(embd, 2) # (bs, dialog_len, embd)\n",
    "        x = torch.cat((embd, context, bow, prev), 2) # (bs, dialog_len, embd+context_dim)\n",
    "        x, (h, c) = self.lstm(x) # (bs, dialog_len, hid), ((1, bs, hid), (1, bs, hid))\n",
    "        y = self.fc(F.tanh(x)) # (bs, dialog_len, action_size)\n",
    "        y = F.softmax(y, -1) # (bs, dialog_len, action_size)\n",
    "        y = y * act_filter\n",
    "        return y\n",
    "\n",
    "    \n",
    "model = HybridCodeNetwork(vocab_size=len(vocab), embd_size=300, hidden_size=128, action_size=len(system_acts), \n",
    "                          context_size=len(entities.keys()), pre_embd_w=pre_embd_w)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next loading a couple of helper functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(data, default_val, maxlen):\n",
    "    for i, d in enumerate(data):\n",
    "        pad_len = maxlen - len(d)\n",
    "        for _ in range(pad_len):\n",
    "            data[i].append([default_val] * len(entities.keys()))\n",
    "    return to_var(torch.FloatTensor(data))\n",
    "\n",
    "\n",
    "def categorical_cross_entropy(preds, labels):\n",
    "    loss = Variable(torch.zeros(1))\n",
    "    for p, label in zip(preds, labels):\n",
    "        loss -= torch.log(p[label] + 1.e-7).cpu()\n",
    "    loss /= preds.size(0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the data for each batch is loaded, the user utterances and system actions are vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word_vector(uttrs_list, w2i, dialog_maxlen, uttr_maxlen):\n",
    "    # returns batch of lists of word indices (as defined in w2i)\n",
    "    dialog_list = []\n",
    "    for uttrs in uttrs_list:\n",
    "        dialog = []\n",
    "        for sentence in uttrs:\n",
    "            sent_vec = [w2i[w] if w in w2i else w2i[UNK] for w in sentence]\n",
    "            sent_vec = add_padding(sent_vec, uttr_maxlen)\n",
    "            dialog.append(sent_vec)\n",
    "        for _ in range(dialog_maxlen - len(dialog)):\n",
    "            dialog.append([0] * uttr_maxlen)\n",
    "        dialog = torch.LongTensor(dialog[:dialog_maxlen])\n",
    "        dialog_list.append(dialog)\n",
    "    return to_var(torch.stack(dialog_list, 0))\n",
    "\n",
    "\n",
    "def get_data_from_batch(batch, w2i, act2i):\n",
    "    # vectorises input data\n",
    "    uttrs_list = [d[0] for d in batch]\n",
    "    dialog_maxlen = max([len(uttrs) for uttrs in uttrs_list])\n",
    "    uttr_maxlen = max([len(u) for uttrs in uttrs_list for u in uttrs])\n",
    "    uttr_var = make_word_vector(uttrs_list, w2i, dialog_maxlen, uttr_maxlen)\n",
    "\n",
    "    batch_labels = [d[1] for d in batch]\n",
    "    labels_var = []\n",
    "    for labels in batch_labels:\n",
    "        vec_labels = [act2i[l] for l in labels]\n",
    "        pad_len = dialog_maxlen - len(labels)\n",
    "        for _ in range(pad_len):\n",
    "            vec_labels.append(act2i[SILENT])\n",
    "        labels_var.append(torch.LongTensor(vec_labels))\n",
    "    labels_var = to_var(torch.stack(labels_var, 0))\n",
    "\n",
    "    batch_prev_acts = [d[4] for d in batch]\n",
    "    prev_var = []\n",
    "    for prev_acts in batch_prev_acts:\n",
    "        vec_prev_acts = []\n",
    "        for act in prev_acts:\n",
    "            tmp = [0] * len(act2i)\n",
    "            tmp[act2i[act]] = 1\n",
    "            vec_prev_acts.append(tmp)\n",
    "        pad_len = dialog_maxlen - len(prev_acts)\n",
    "        for _ in range(pad_len):\n",
    "            vec_prev_acts.append([0] * len(act2i))\n",
    "        prev_var.append(torch.FloatTensor(vec_prev_acts))\n",
    "    prev_var = to_var(torch.stack(prev_var, 0))\n",
    "\n",
    "    context = copy.deepcopy([d[2] for d in batch])\n",
    "    context = padding(context, 1, dialog_maxlen)\n",
    "\n",
    "    bow = copy.deepcopy([d[3] for d in batch])\n",
    "    bow = padding(bow, 0, dialog_maxlen)\n",
    "\n",
    "    act_filter = copy.deepcopy([d[5] for d in batch])\n",
    "    act_filter = padding(act_filter, 0, dialog_maxlen)\n",
    "\n",
    "    return uttr_var, labels_var, context, bow, prev_var, act_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we are ready to train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Train---\n",
      "Epoch 0\n",
      "Acc: 0.000% (0/2)\n",
      "loss 9.522615432739258\n",
      "Acc: 35.639% (170/477)\n",
      "loss 1.6240317821502686\n",
      "Acc: 47.953% (445/928)\n",
      "loss 0.6303272843360901\n",
      "Acc: 56.516% (798/1412)\n",
      "loss 1.1948012113571167\n",
      "Acc: 61.983% (1169/1886)\n",
      "loss 0.30662867426872253\n",
      "Acc: 65.693% (1551/2361)\n",
      "loss 0.32327380776405334\n",
      "Acc: 68.087% (1901/2792)\n",
      "loss 8.12259292602539\n",
      "Acc: 70.227% (2262/3221)\n",
      "loss 0.06248326227068901\n",
      "Acc: 71.912% (2637/3667)\n",
      "loss 0.048547301441431046\n",
      "Acc: 73.522% (3035/4128)\n",
      "loss 0.09099173545837402\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, optimizer, w2i, act2i, n_epochs=2, batch_size=1):\n",
    "    print('----Train---')\n",
    "    data = copy.copy(data)\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch', epoch)\n",
    "        random.shuffle(data)\n",
    "        acc, total = 0, 0\n",
    "        for batch_idx in range(0, len(data)-batch_size, batch_size):\n",
    "            batch = data[batch_idx:batch_idx+batch_size]\n",
    "            uttrs, labels, contexts, bows, prevs, act_fils = get_data_from_batch(batch, w2i, act2i)\n",
    "            preds = model(uttrs, contexts, bows, prevs, act_fils)\n",
    "            action_size = preds.size(-1)\n",
    "            preds = preds.view(-1, action_size)\n",
    "            labels = labels.view(-1)\n",
    "            loss = categorical_cross_entropy(preds, labels)\n",
    "            acc += torch.sum(labels == torch.max(preds, 1)[1]).data[0]\n",
    "            total += labels.size(0)\n",
    "            if batch_idx % (100 * batch_size) == 0:\n",
    "                print('Acc: {:.3f}% ({}/{})'.format(100 * acc/total, acc, total))\n",
    "                print('loss', loss.data[0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "train(model, train_data, optimizer, w2i, act2i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And to test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Test---\n",
      "Test Acc: 87.154% (3969/4554)\n"
     ]
    }
   ],
   "source": [
    "def test(model, data, w2i, act2i, batch_size=1):\n",
    "    print('----Test---')\n",
    "    model.eval()\n",
    "    acc, total = 0, 0\n",
    "    for batch_idx in range(0, len(data)-batch_size, batch_size):\n",
    "        batch = data[batch_idx:batch_idx+batch_size]\n",
    "        uttrs, labels, contexts, bows, prevs, act_fils = get_data_from_batch(batch, w2i, act2i)\n",
    "        preds = model(uttrs, contexts, bows, prevs, act_fils)\n",
    "        action_size = preds.size(-1)\n",
    "        preds = preds.view(-1, action_size)\n",
    "        labels = labels.view(-1)\n",
    "        acc += torch.sum(labels == torch.max(preds, 1)[1]).data[0]\n",
    "        total += labels.size(0)\n",
    "    print('Test Acc: {:.3f}% ({}/{})'.format(100 * acc/total, acc, total))\n",
    "\n",
    "test(model, test_data, w2i, act2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
